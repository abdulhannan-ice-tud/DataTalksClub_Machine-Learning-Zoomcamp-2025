{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4ffb436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import statistics\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6afebb-50ae-429e-9aa2-3a915be20dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78929bfd-3633-4caa-8b4a-e8a405b101aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 85249\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0f83f5c-6379-4031-b380-d9b20f5acf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "CNNBinaryClassifier(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (adaptive_pool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
      "  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 1/10  Loss: 0.6700  Acc: 0.5575  Val Loss: 0.6561  Val Acc: 0.7015\n",
      "Epoch 2/10  Loss: 0.6150  Acc: 0.7100  Val Loss: 0.6096  Val Acc: 0.7065\n",
      "Epoch 3/10  Loss: 0.5633  Acc: 0.7475  Val Loss: 0.5573  Val Acc: 0.7313\n",
      "Epoch 4/10  Loss: 0.5179  Acc: 0.7712  Val Loss: 0.5532  Val Acc: 0.7711\n",
      "Epoch 5/10  Loss: 0.4982  Acc: 0.7825  Val Loss: 0.5318  Val Acc: 0.7662\n",
      "Epoch 6/10  Loss: 0.4791  Acc: 0.7963  Val Loss: 0.5263  Val Acc: 0.7662\n",
      "Epoch 7/10  Loss: 0.4569  Acc: 0.7937  Val Loss: 0.5489  Val Acc: 0.7164\n",
      "Epoch 8/10  Loss: 0.4347  Acc: 0.8075  Val Loss: 0.5163  Val Acc: 0.7562\n",
      "Epoch 9/10  Loss: 0.4389  Acc: 0.8137  Val Loss: 0.5553  Val Acc: 0.7214\n",
      "Epoch 10/10  Loss: 0.4329  Acc: 0.8275  Val Loss: 0.4784  Val Acc: 0.7910\n"
     ]
    }
   ],
   "source": [
    "# reproducibility \n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "# Model \n",
    "class CNNBinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use padding=1 to preserve spatial dims before pooling\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)                          \n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) \n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))  \n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 64)\n",
    "        self.dropout = nn.Dropout(p=0.4)   \n",
    "        self.fc2 = nn.Linear(64, 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits  \n",
    "\n",
    "\n",
    "# instantiate model once and move to device\n",
    "model = CNNBinaryClassifier().to(device)\n",
    "print(model)\n",
    "\n",
    "# criterion and optimizer \n",
    "criterion = nn.BCEWithLogitsLoss()  # stable: combines sigmoid+BCEloss internally\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
    "\n",
    "# Data transforms \n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "val_transforms = train_transforms\n",
    "\n",
    "# dataset paths \n",
    "train_dir = 'data/train'\n",
    "test_dir  = 'data/test'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transforms)\n",
    "validation_dataset = datasets.ImageFolder(root=test_dir, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "\n",
    "# training loop \n",
    "num_epochs = 10\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)           \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # compute predicted labels from logits\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        predicted = (probs > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "# validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predicted = (probs > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  Loss: {epoch_loss:.4f}  Acc: {epoch_acc:.4f}  Val Loss: {val_epoch_loss:.4f}  Val Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "# Save After training  :\n",
    "torch.save(model.state_dict(), \"cnn_binary_fixed.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "173626df-2742-4493-aa94-b7c709d4bf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median training accuracy: 0.788125\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "median_acc = statistics.median(history['acc'])\n",
    "print(\"Median training accuracy:\", median_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "757d10d6-5c35-44d8-be45-c488fd4991d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample std dev: 0.15445989770811064\n",
      "population std dev: 0.1465335251742754\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "losses = [0.6586,0.5467,0.5127,0.4621,0.4185,0.3805,0.2969,0.2143,0.2472,0.1975]\n",
    "print(\"sample std dev:\", statistics.stdev(losses))\n",
    "print(\"population std dev:\", statistics.pstdev(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc43597f-ccdb-4b2a-b15a-66ecd788bc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUG] Epoch 1/10  Loss: 0.5090  Acc: 0.7775  Val Loss: 0.4730  Val Acc: 0.7910\n",
      "[AUG] Epoch 2/10  Loss: 0.4949  Acc: 0.7712  Val Loss: 0.4491  Val Acc: 0.7960\n",
      "[AUG] Epoch 3/10  Loss: 0.4709  Acc: 0.7875  Val Loss: 0.4618  Val Acc: 0.7662\n",
      "[AUG] Epoch 4/10  Loss: 0.4847  Acc: 0.7887  Val Loss: 0.4521  Val Acc: 0.7662\n",
      "[AUG] Epoch 5/10  Loss: 0.4749  Acc: 0.7662  Val Loss: 0.4661  Val Acc: 0.7861\n",
      "[AUG] Epoch 6/10  Loss: 0.4716  Acc: 0.7900  Val Loss: 0.4485  Val Acc: 0.7711\n",
      "[AUG] Epoch 7/10  Loss: 0.4544  Acc: 0.7875  Val Loss: 0.5135  Val Acc: 0.7512\n",
      "[AUG] Epoch 8/10  Loss: 0.4707  Acc: 0.7987  Val Loss: 0.5112  Val Acc: 0.7512\n",
      "[AUG] Epoch 9/10  Loss: 0.4580  Acc: 0.8037  Val Loss: 0.4339  Val Acc: 0.7861\n",
      "[AUG] Epoch 10/10  Loss: 0.4465  Acc: 0.8037  Val Loss: 0.4832  Val Acc: 0.7612\n"
     ]
    }
   ],
   "source": [
    "# Augmented train transforms (as requested)\n",
    "aug_train_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.RandomRotation(50),\n",
    "    transforms.RandomResizedCrop(200, scale=(0.9,1.0), ratio=(0.9,1.1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "\n",
    "# Replace the train dataset and loader \n",
    "train_dataset.transform = aug_train_transforms\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "\n",
    "# Continue training for 10 more epochs\n",
    "num_more_epochs = 10\n",
    "\n",
    "for epoch in range(num_more_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        predicted = (probs > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "    # validation (test) step\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predicted = (probs > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"[AUG] Epoch {epoch+1}/{num_more_epochs}  Loss: {epoch_loss:.4f}  Acc: {epoch_acc:.4f}  Val Loss: {val_epoch_loss:.4f}  Val Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e2ae01b-dea2-478a-8d71-e0900799db9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test (val) loss over all recorded epochs: 0.5112702000422857\n"
     ]
    }
   ],
   "source": [
    "# After training, metrics for Question 5\n",
    "# Mean of test loss for all epochs\n",
    "mean_test_loss_all_epochs = float(np.mean(history['val_loss']))\n",
    "print(\"Mean test (val) loss over all recorded epochs:\", mean_test_loss_all_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "771cf4c6-b13d-4920-8855-165d0c2fc4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average val accuracy for last 5 epochs of augmentation block: 0.7641791044776121\n"
     ]
    }
   ],
   "source": [
    "# Q6: average of test accuracy for the last 5 epochs. \n",
    "aug_block_val_acc = history['val_acc'][-10:]   # last 10 epochs (augmentation block)\n",
    "avg_last5_aug = float(np.mean(aug_block_val_acc[-5:]))\n",
    "print(\"Average val accuracy for last 5 epochs of augmentation block:\", avg_last5_aug)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
